{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from datasets import load_metric\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer,AutoConfig\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "import torch\n",
    "import re\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bson.objectid import ObjectId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.auto.modeling_auto.AutoModelForTokenClassification"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AutoModelForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-MedicationForm',\n",
       " 'I-MedicationForm',\n",
       " 'B-Age',\n",
       " 'I-Age',\n",
       " 'B-Frequency',\n",
       " 'I-Frequency',\n",
       " 'B-MedicationRoute',\n",
       " 'I-MedicationRoute',\n",
       " 'B-MedicationClass',\n",
       " 'I-MedicationClass',\n",
       " 'B-MeasurementUnit',\n",
       " 'I-MeasurementUnit',\n",
       " 'B-GeneOrProtein',\n",
       " 'I-GeneOrProtein',\n",
       " 'B-Course',\n",
       " 'I-Course',\n",
       " 'B-Gender',\n",
       " 'I-Gender',\n",
       " 'B-Dosage',\n",
       " 'I-Dosage',\n",
       " 'B-ConditionScale',\n",
       " 'I-ConditionScale',\n",
       " 'B-MedicationName',\n",
       " 'I-MedicationName',\n",
       " 'B-MeasurementValue',\n",
       " 'I-MeasurementValue',\n",
       " 'B-Allergen',\n",
       " 'I-Allergen',\n",
       " 'B-RelationalOperator',\n",
       " 'I-RelationalOperator',\n",
       " 'B-Diagnosis',\n",
       " 'I-Diagnosis',\n",
       " 'B-BodyStructure',\n",
       " 'I-BodyStructure',\n",
       " 'B-ConditionQualifier',\n",
       " 'I-ConditionQualifier',\n",
       " 'B-CareEnvironment',\n",
       " 'I-CareEnvironment',\n",
       " 'B-AdministrativeEvent',\n",
       " 'I-AdministrativeEvent',\n",
       " 'B-Time',\n",
       " 'I-Time',\n",
       " 'B-FamilyRelation',\n",
       " 'I-FamilyRelation',\n",
       " 'B-MutationType',\n",
       " 'I-MutationType',\n",
       " 'B-ExaminationName',\n",
       " 'I-ExaminationName',\n",
       " 'B-Variant',\n",
       " 'I-Variant',\n",
       " 'B-TreatmentName',\n",
       " 'I-TreatmentName',\n",
       " 'B-Direction',\n",
       " 'I-Direction',\n",
       " 'B-Expression',\n",
       " 'I-Expression',\n",
       " 'B-SymptomOrSign',\n",
       " 'I-SymptomOrSign',\n",
       " 'B-HealthcareProfession',\n",
       " 'I-HealthcareProfession',\n",
       " 'O']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r\"C:\\Downloads\\Train.csv\")\n",
    "L = [x for x in list(df.columns) if \"_\" not in x]\n",
    "label_list = []\n",
    "for m in L:\n",
    "    label_list.append(\"B-\"+m)\n",
    "    label_list.append(\"I-\"+m)\n",
    "label_list.append(\"O\")\n",
    "label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MedicationForm',\n",
       " 'Age',\n",
       " 'Frequency',\n",
       " 'MedicationRoute',\n",
       " 'MedicationClass',\n",
       " 'MeasurementUnit',\n",
       " 'GeneOrProtein',\n",
       " 'Course',\n",
       " 'Gender',\n",
       " 'Dosage',\n",
       " 'ConditionScale',\n",
       " 'MedicationName',\n",
       " 'MeasurementValue',\n",
       " 'Allergen',\n",
       " 'RelationalOperator',\n",
       " 'Diagnosis',\n",
       " 'BodyStructure',\n",
       " 'ConditionQualifier',\n",
       " 'CareEnvironment',\n",
       " 'AdministrativeEvent',\n",
       " 'Time',\n",
       " 'FamilyRelation',\n",
       " 'MutationType',\n",
       " 'ExaminationName',\n",
       " 'Variant',\n",
       " 'TreatmentName',\n",
       " 'Direction',\n",
       " 'Expression',\n",
       " 'SymptomOrSign',\n",
       " 'HealthcareProfession']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B-MedicationForm': 0,\n",
       " 'I-MedicationForm': 1,\n",
       " 'B-Age': 2,\n",
       " 'I-Age': 3,\n",
       " 'B-Frequency': 4,\n",
       " 'I-Frequency': 5,\n",
       " 'B-MedicationRoute': 6,\n",
       " 'I-MedicationRoute': 7,\n",
       " 'B-MedicationClass': 8,\n",
       " 'I-MedicationClass': 9,\n",
       " 'B-MeasurementUnit': 10,\n",
       " 'I-MeasurementUnit': 11,\n",
       " 'B-GeneOrProtein': 12,\n",
       " 'I-GeneOrProtein': 13,\n",
       " 'B-Course': 14,\n",
       " 'I-Course': 15,\n",
       " 'B-Gender': 16,\n",
       " 'I-Gender': 17,\n",
       " 'B-Dosage': 18,\n",
       " 'I-Dosage': 19,\n",
       " 'B-ConditionScale': 20,\n",
       " 'I-ConditionScale': 21,\n",
       " 'B-MedicationName': 22,\n",
       " 'I-MedicationName': 23,\n",
       " 'B-MeasurementValue': 24,\n",
       " 'I-MeasurementValue': 25,\n",
       " 'B-Allergen': 26,\n",
       " 'I-Allergen': 27,\n",
       " 'B-RelationalOperator': 28,\n",
       " 'I-RelationalOperator': 29,\n",
       " 'B-Diagnosis': 30,\n",
       " 'I-Diagnosis': 31,\n",
       " 'B-BodyStructure': 32,\n",
       " 'I-BodyStructure': 33,\n",
       " 'B-ConditionQualifier': 34,\n",
       " 'I-ConditionQualifier': 35,\n",
       " 'B-CareEnvironment': 36,\n",
       " 'I-CareEnvironment': 37,\n",
       " 'B-AdministrativeEvent': 38,\n",
       " 'I-AdministrativeEvent': 39,\n",
       " 'B-Time': 40,\n",
       " 'I-Time': 41,\n",
       " 'B-FamilyRelation': 42,\n",
       " 'I-FamilyRelation': 43,\n",
       " 'B-MutationType': 44,\n",
       " 'I-MutationType': 45,\n",
       " 'B-ExaminationName': 46,\n",
       " 'I-ExaminationName': 47,\n",
       " 'B-Variant': 48,\n",
       " 'I-Variant': 49,\n",
       " 'B-TreatmentName': 50,\n",
       " 'I-TreatmentName': 51,\n",
       " 'B-Direction': 52,\n",
       " 'I-Direction': 53,\n",
       " 'B-Expression': 54,\n",
       " 'I-Expression': 55,\n",
       " 'B-SymptomOrSign': 56,\n",
       " 'I-SymptomOrSign': 57,\n",
       " 'B-HealthcareProfession': 58,\n",
       " 'I-HealthcareProfession': 59,\n",
       " 'O': 60}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_encoding_dict = {}\n",
    "for i in range(len(label_list)):\n",
    "    label_encoding_dict[label_list[i]] = i\n",
    "label_encoding_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "task = \"ner\" \n",
    "model_checkpoint = \"d4data/biomedical-ner-all\"\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Pre_process_clean(text):\n",
    "    try :\n",
    "        text = re.sub(r'\\(','',text)\n",
    "        text = re.sub(r\"\\)\",'',text)\n",
    "        text = re.sub(r\"\\.\",'',text)\n",
    "        text = re.sub(r\"|\",'',text)\n",
    "        text = re.sub(r'[-#&@*/\\',:;\"]','',text)\n",
    "        return text\n",
    "    except:\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def List_Tokenizer(text):\n",
    "    tokens = tokenizer(text)\n",
    "    words = tokenizer.batch_decode(tokens['input_ids'])\n",
    "    return words\n",
    "\n",
    "def Tokenizer_Label(Label_words):\n",
    "    L = []\n",
    "    try :\n",
    "        if Label_words[0] == 'n':\n",
    "            return ''\n",
    "        for x in Label_words:\n",
    "            w = List_Tokenizer(x)\n",
    "            for i in w:\n",
    "                if (i != '[CLS]') and (i != '[SEP]'):\n",
    "                    L.append(i)\n",
    "    except :\n",
    "        return ''\n",
    "    return L\n",
    "\n",
    "def Ner_tagging(T,MT):\n",
    "    T['ner_tags'] = 0\n",
    "    for i in range(T.shape[0]):\n",
    "        print(f\"Sent : {T['original_text'][i]}\")\n",
    "        NER = []\n",
    "        L = T['tokens'][i]\n",
    "        Master_Dic_words = {}\n",
    "        Master_Dic_words_tag = {}\n",
    "        JT = []\n",
    "        for k in MT:\n",
    "            j = 0\n",
    "            for tp in str(T[k][i]).split(\"||\"):\n",
    "                JT.append(k+\"_\"+str(j))\n",
    "                Master_Dic_words[k+\"_\"+str(j)+\"_words\"] = [t.lower() for t in str(tp).split()]\n",
    "                Master_Dic_words_tag[k+\"_\"+str(j)+\"_words_tag\"] = Tokenizer_Label(Master_Dic_words[k+\"_\"+str(j)+\"_words\"])\n",
    "                j = j + 1\n",
    "        Master_Dic_Start = {}\n",
    "        Master_Dic_Sec = {}\n",
    "        for k in JT:\n",
    "            Master_Dic_Start[\"Start_\"+k] = Master_Dic_words_tag[k+\"_words_tag\"][0]\n",
    "            try :\n",
    "                Master_Dic_Sec[\"Sec_\"+k] = Master_Dic_words_tag[k+\"_words_tag\"][1]\n",
    "            except :\n",
    "                Master_Dic_Sec[\"Sec_\"+k] = \"\"\n",
    "                \n",
    "        prev_tag = \"\"\n",
    "        k = 0\n",
    "        for j in L:\n",
    "            f = 0\n",
    "            first = \"Yes\"\n",
    "            for mt in JT:\n",
    "                mtn = mt.split(\"_\")[0].strip()\n",
    "                if first == \"Yes\":\n",
    "                    first = \"No\"\n",
    "                    if j.lower() in Master_Dic_words_tag[mt+\"_words_tag\"]:\n",
    "                        f = 1\n",
    "                        if (prev_tag == \"B-\"+mtn) or (prev_tag == \"I-\"+mtn):\n",
    "                            NER.append('I-'+mtn)\n",
    "                            prev_tag = \"I-\"+mtn\n",
    "                        else :\n",
    "                            if j.lower() == Master_Dic_Start[\"Start_\"+mt] and len(Master_Dic_words_tag[mt+\"_words_tag\"]) == 1:\n",
    "                                NER.append('B-'+mtn)\n",
    "                                prev_tag = \"B-\"+mtn\n",
    "                                Start_Ent = \"NAN\"\n",
    "                            elif j.lower() == Master_Dic_Start[\"Start_\"+mt] and (L[k+1].lower() == Master_Dic_Sec[\"Sec_\"+mt] or L[k+1].lower() == \"\"):\n",
    "                                NER.append('B-'+mtn)\n",
    "                                prev_tag = \"B-\"+mtn\n",
    "                                Start_Ent = \"NAN\"\n",
    "                            else :\n",
    "                                f = 0\n",
    "                else :\n",
    "                    if j.lower() in Master_Dic_words_tag[mt+\"_words_tag\"] and f == 0:\n",
    "                        f = 1\n",
    "                        if (prev_tag == \"B-\"+mtn) or (prev_tag == \"I-\"+mtn):\n",
    "                            NER.append('I-'+mtn)\n",
    "                            prev_tag = \"I-\"+mtn\n",
    "                        else :\n",
    "                            if j.lower() == Master_Dic_Start[\"Start_\"+mt]  and (L[k+1].lower() == Master_Dic_Sec[\"Sec_\"+mt]  or L[k+1].lower() == \"\"):\n",
    "                                NER.append('B-'+mtn)\n",
    "                                prev_tag = \"B-\"+mtn\n",
    "                                Start_HOP = \"NAN\"\n",
    "                            else :\n",
    "                                f = 0\n",
    "\n",
    "            if f == 0 :\n",
    "                NER.append('O')\n",
    "                prev_tag = \"O\"\n",
    "            k = k + 1\n",
    "        T['ner_tags'][i] = str(NER)\n",
    "    return T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "T = pd.read_csv(r\"C:\\Downloads\\Train.csv\")\n",
    "T.replace('0','NAN', inplace=True)\n",
    "T.replace('','NAN', inplace=True)\n",
    "T.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "T.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "T['tokens'] = T['original_text'].apply(lambda x : List_Tokenizer(str(x)))\n",
    "T = Ner_tagging(T,L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "T.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ti = T[T['ner_tags']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ti.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "T['original_text'][8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "T['tokens'][8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "T['ner_tags'][8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Train = T[['ner_tags','tokens']]\n",
    "Train.head()\n",
    "Train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_un_token_dataset(train_df, test_df):\n",
    "    train_dataset = Dataset.from_pandas(train_df)\n",
    "    test_dataset = Dataset.from_pandas(test_df)\n",
    "    return (train_dataset, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Train = Train.sample(frac=1, axis=1).sample(frac=1).reset_index(drop=True)\n",
    "Train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Train['ner_tags'] = Train['ner_tags'].apply(lambda x : ast.literal_eval(str(x)))\n",
    "Train['tokens'] = Train['tokens'].apply(lambda x : ast.literal_eval(str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for row in range(Train.shape[0]):\n",
    "    if len(Train['ner_tags'][row]) == len(Train['tokens'][row]):\n",
    "        continue\n",
    "    else :\n",
    "        print(f\"Error in row number : {row}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = Train.iloc[:10000,:]\n",
    "test_df = Train.iloc[10000:,:]\n",
    "train_dataset, test_dataset = get_un_token_dataset(train_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dataset.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    label_all_tokens = True\n",
    "    tokenized_inputs = tokenizer(list(examples[\"tokens\"]), truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif label[word_idx] == '0':\n",
    "                label_ids.append(0)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label_encoding_dict[label[word_idx]])\n",
    "            else:\n",
    "                label_ids.append(label_encoding_dict[label[word_idx]] if label_all_tokens else -100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "        \n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_tokenized = train_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "test_tokenized = test_dataset.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This Method uses the config but uses random weights instead of pretrained weights of BioBert-NER\n",
    "\"\"\"\n",
    "config = AutoConfig.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This Method removes top Classification layer and uses pretrained weights of BioBert-NER \n",
    "\"\"\"\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint)\n",
    "model.classifier = torch.nn.Linear(in_features=model.classifier.in_features, out_features=config.num_labels, bias=True)\n",
    "model.config = config    \n",
    "model.num_labels = config.num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    f\"test-{task}\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=13,\n",
    "    weight_decay=1e-5,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "metric = load_metric(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [[label_list[p] for (p, l) in zip(prediction, label) if l != -100] for prediction, label in zip(predictions, labels)]\n",
    "    true_labels = [[label_list[l] for (p, l) in zip(prediction, label) if l != -100] for prediction, label in zip(predictions, labels)]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\"precision\": results[\"overall_precision\"], \"recall\": results[\"overall_recall\"], \"f1\": results[\"overall_f1\"], \"accuracy\": results[\"overall_accuracy\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_tokenized,\n",
    "    eval_dataset=test_tokenized,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.evaluate()\n",
    "trainer.save_model('BIONER-MODEL-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def clean(text):\n",
    "    try :\n",
    "        text = re.sub(r'[-#&@*/{}():;.,]','',text)\n",
    "        return text\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_scispacy_tags(text):\n",
    "\n",
    "        text = clean(text)\n",
    "        T = []\n",
    "        df = pd.read_csv(r\"C:\\Downloads\\BIONER-DOCKER\\Medication.csv\")\n",
    "        L = list(df['Medication'])\n",
    "        for j in L:\n",
    "            j = clean(j)\n",
    "            if j.strip().upper() in text.upper() and len(j) > 3 and len(j.strip()) > 0:\n",
    "                j_words = j.strip().upper().split()\n",
    "                text_words = text.upper().split()\n",
    "                f = 1\n",
    "                for wd in j_words:\n",
    "                    if wd not in text_words:\n",
    "                        f = 0\n",
    "                        break\n",
    "                if f == 1:\n",
    "                    T.append([\"MedicationName\",j.strip(),len(j.strip())])\n",
    "        df = pd.read_csv(r\"C:\\Downloads\\BIONER-DOCKER\\Diagnosis.csv\")\n",
    "        L = list(df['Diagnosis'])\n",
    "        for j in L:\n",
    "            j = clean(j)\n",
    "            if j.strip().upper() in text.upper() and len(j) > 3 and len(j.strip()) > 0:\n",
    "                j_words = j.strip().upper().split()\n",
    "                text_words = text.upper().split()\n",
    "                f = 1\n",
    "                for wd in j_words:\n",
    "                    if wd not in text_words:\n",
    "                        f = 0\n",
    "                        break\n",
    "                if f == 1:\n",
    "                    T.append([\"Diagnosis\",j.strip(),len(j.strip())])\n",
    "        M = sorted(T,key=lambda x : x[2],reverse=True)\n",
    "        Z = [x[:2] for x in M]\n",
    "        return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "D = pd.read_csv(r\"C:\\Downloads\\training.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('./BIONER-MODEL/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "L1_sentences = []\n",
    "Preds = []\n",
    "Texts = []\n",
    "model = AutoModelForTokenClassification.from_pretrained('./BIONER-MODEL/')\n",
    "for paragraph in D['original_text']:\n",
    "    Texts.append(paragraph)\n",
    "#     paragraph = Pre_process_clean(paragraph)\n",
    "    tokens = tokenizer(paragraph)\n",
    "    torch.tensor(tokens['input_ids']).unsqueeze(0).size()\n",
    "\n",
    "    predictions = model.forward(input_ids=torch.tensor(tokens['input_ids']).unsqueeze(0), attention_mask=torch.tensor(tokens['attention_mask']).unsqueeze(0))\n",
    "    predictions = torch.argmax(predictions.logits.squeeze(), axis=1)\n",
    "    predictions = [label_list[i] for i in predictions]\n",
    "\n",
    "    words = tokenizer.batch_decode(tokens['input_ids'])\n",
    "    L1_sentences.append(words)\n",
    "    Preds.append(predictions)\n",
    "E = pd.DataFrame(list(zip(L1_sentences,Preds)),columns=[\"L1_Sents\",\"Preds\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for mt in L:\n",
    "    C[mt] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def entity_identification(words,predictions,tag1,tag2):\n",
    "    NER =[]\n",
    "    NER_tag = []\n",
    "    Pred_Lab = []\n",
    "    Pred_tag = []\n",
    "    prev_tag = ''\n",
    "    ent = ''\n",
    "    tag = ''\n",
    "    start_ind = 0\n",
    "    end_ind = 0\n",
    "#     print(f\"Words : {words}\")\n",
    "#     print(f\"Predictions : {predictions}\")\n",
    "    for i in range(len(predictions)-1):\n",
    "        if (predictions[i] == tag1 or predictions[i] == tag2) and (tag != tag1):\n",
    "            start_ind = i\n",
    "#             print(f\"start_ind : {start_ind}\")\n",
    "            tag = tag1\n",
    "        if ((predictions[i] == tag2 or predictions[i] == tag1 or '##' in words[i]) and (predictions[i+1] != tag2 and ('##' not in words[i+1]))  and (tag == tag1)):\n",
    "            end_ind = i\n",
    "#             print(f\"end_ind : {end_ind}\")\n",
    "            tag = tag2\n",
    "        if (tag == tag2) and (end_ind != 0):\n",
    "            while ('##' in words[start_ind]):\n",
    "                start_ind = start_ind - 1\n",
    "            for j in range(start_ind,end_ind+1):\n",
    "#                 if words[j] not in NER:\n",
    "                NER.append(words[j])\n",
    "                if j == start_ind:\n",
    "                    Pred_tag.append(tag1)\n",
    "                else :\n",
    "                    Pred_tag.append(predictions[j])\n",
    "#                     Pred_tag[words[j]] = predictions[j]\n",
    "\n",
    "            start_ind = 0\n",
    "            end_ind = 0\n",
    "            tag = ''\n",
    "#         if (predictions[i] == 'B-RP') or (predictions[i] == 'I-RP'):\n",
    "#             NER.append(words[i])\n",
    "#     print(f\"NER : {NER}\")\n",
    "#     print(f\"Pred_tag : {Pred_tag}\")\n",
    "    if len(NER) != 0:\n",
    "        k = 0\n",
    "        for j in NER:\n",
    "            if ('##' in j) or (Pred_tag[k] != tag1):\n",
    "                if ('##' in j):\n",
    "                    ent = ent + j.replace('##', '')\n",
    "                else :\n",
    "                    ent = ent + \" \" + j\n",
    "                prev_tag = '##'\n",
    "            else :\n",
    "                if prev_tag == '##' or prev_tag == 'N#':\n",
    "                    NER_tag.append(ent)\n",
    "                    Ent = ent\n",
    "                    if (Ent not in Pred_Lab and Ent != \"\"):\n",
    "                        Pred_Lab.append(Ent)\n",
    "                ent = j\n",
    "                prev_tag = 'N#'\n",
    "            k = k + 1\n",
    "        NER_tag.append(ent)\n",
    "        Ent = ent\n",
    "        Pred_Lab_words = [sub.split() for sub in Pred_Lab]\n",
    "        if (Ent not in Pred_Lab and Ent not in Pred_Lab_words and Ent != \"\"):\n",
    "            Pred_Lab.append(Ent)\n",
    "        Pred_Lab_clean = []\n",
    "        for e in range(len(Pred_Lab)):\n",
    "            f = 0\n",
    "            for k in range(e+1,len(Pred_Lab)):\n",
    "                if Pred_Lab[e] in Pred_Lab[k].split():\n",
    "                    f = 1\n",
    "            if f == 0:\n",
    "                Pred_Lab_clean.append(Pred_Lab[e])\n",
    "#         print(f\"Pred_Lab : {Pred_Lab_clean}\")\n",
    "        A = \",\".join([x for x in Pred_Lab_clean])\n",
    "    else :\n",
    "        A = \"\"\n",
    "    return str(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "words = list(E['L1_Sents'])\n",
    "predictions = list(E['Preds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(words)):\n",
    "    for mt in L:\n",
    "        A = entity_identification(words[i],predictions[i],'B-'+mt,'I-'+mt)\n",
    "        if mt == \"Diagnosis\" or mt == \"MedicationName\":\n",
    "            text = Texts[i]\n",
    "            T = get_scispacy_tags(text)\n",
    "            B = \"\"\n",
    "            for G in T:\n",
    "                if G[0] == mt:\n",
    "                    B = B + \",\" + G[1]\n",
    "            A = B + \",\" + A\n",
    "        C[mt][i] = A\n",
    "#         Master_Entities[mt].append(A)\n",
    "#         print(\"------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
